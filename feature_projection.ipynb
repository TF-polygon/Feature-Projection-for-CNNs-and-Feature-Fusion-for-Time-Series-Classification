{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78aa5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsmoothie.smoother import *\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyts.image import GramianAngularField, RecurrencePlot, MarkovTransitionField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8032cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Future_Close</th>\n",
       "      <th>Return</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 22:00</td>\n",
       "      <td>1.12106</td>\n",
       "      <td>1.12166</td>\n",
       "      <td>1.12106</td>\n",
       "      <td>1.12143</td>\n",
       "      <td>1215</td>\n",
       "      <td>1.11713</td>\n",
       "      <td>-0.003834</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 23:00</td>\n",
       "      <td>1.12143</td>\n",
       "      <td>1.12218</td>\n",
       "      <td>1.12142</td>\n",
       "      <td>1.12188</td>\n",
       "      <td>1407</td>\n",
       "      <td>1.11708</td>\n",
       "      <td>-0.004279</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-02 0:00</td>\n",
       "      <td>1.12188</td>\n",
       "      <td>1.12190</td>\n",
       "      <td>1.12157</td>\n",
       "      <td>1.12183</td>\n",
       "      <td>1790</td>\n",
       "      <td>1.11754</td>\n",
       "      <td>-0.003824</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-02 1:00</td>\n",
       "      <td>1.12182</td>\n",
       "      <td>1.12244</td>\n",
       "      <td>1.12180</td>\n",
       "      <td>1.12209</td>\n",
       "      <td>3135</td>\n",
       "      <td>1.11735</td>\n",
       "      <td>-0.004224</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-02 2:00</td>\n",
       "      <td>1.12210</td>\n",
       "      <td>1.12245</td>\n",
       "      <td>1.12184</td>\n",
       "      <td>1.12222</td>\n",
       "      <td>2121</td>\n",
       "      <td>1.11725</td>\n",
       "      <td>-0.004429</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33684</th>\n",
       "      <td>2025-07-09 22:00</td>\n",
       "      <td>1.17212</td>\n",
       "      <td>1.17249</td>\n",
       "      <td>1.17211</td>\n",
       "      <td>1.17243</td>\n",
       "      <td>527</td>\n",
       "      <td>1.17055</td>\n",
       "      <td>-0.001604</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33685</th>\n",
       "      <td>2025-07-09 23:00</td>\n",
       "      <td>1.17244</td>\n",
       "      <td>1.17396</td>\n",
       "      <td>1.17228</td>\n",
       "      <td>1.17378</td>\n",
       "      <td>998</td>\n",
       "      <td>1.17018</td>\n",
       "      <td>-0.003067</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33686</th>\n",
       "      <td>2025-07-10 0:00</td>\n",
       "      <td>1.17375</td>\n",
       "      <td>1.17494</td>\n",
       "      <td>1.17354</td>\n",
       "      <td>1.17436</td>\n",
       "      <td>4514</td>\n",
       "      <td>1.16746</td>\n",
       "      <td>-0.005876</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33687</th>\n",
       "      <td>2025-07-10 1:00</td>\n",
       "      <td>1.17436</td>\n",
       "      <td>1.17443</td>\n",
       "      <td>1.17378</td>\n",
       "      <td>1.17406</td>\n",
       "      <td>2959</td>\n",
       "      <td>1.16750</td>\n",
       "      <td>-0.005587</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33688</th>\n",
       "      <td>2025-07-10 2:00</td>\n",
       "      <td>1.17406</td>\n",
       "      <td>1.17445</td>\n",
       "      <td>1.17372</td>\n",
       "      <td>1.17389</td>\n",
       "      <td>3614</td>\n",
       "      <td>1.16710</td>\n",
       "      <td>-0.005784</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33689 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date     Open     High      Low    Close  Volume  \\\n",
       "0      2020-01-01 22:00  1.12106  1.12166  1.12106  1.12143    1215   \n",
       "1      2020-01-01 23:00  1.12143  1.12218  1.12142  1.12188    1407   \n",
       "2       2020-01-02 0:00  1.12188  1.12190  1.12157  1.12183    1790   \n",
       "3       2020-01-02 1:00  1.12182  1.12244  1.12180  1.12209    3135   \n",
       "4       2020-01-02 2:00  1.12210  1.12245  1.12184  1.12222    2121   \n",
       "...                 ...      ...      ...      ...      ...     ...   \n",
       "33684  2025-07-09 22:00  1.17212  1.17249  1.17211  1.17243     527   \n",
       "33685  2025-07-09 23:00  1.17244  1.17396  1.17228  1.17378     998   \n",
       "33686   2025-07-10 0:00  1.17375  1.17494  1.17354  1.17436    4514   \n",
       "33687   2025-07-10 1:00  1.17436  1.17443  1.17378  1.17406    2959   \n",
       "33688   2025-07-10 2:00  1.17406  1.17445  1.17372  1.17389    3614   \n",
       "\n",
       "       Future_Close    Return  Label  \n",
       "0           1.11713 -0.003834     -1  \n",
       "1           1.11708 -0.004279     -1  \n",
       "2           1.11754 -0.003824     -1  \n",
       "3           1.11735 -0.004224     -1  \n",
       "4           1.11725 -0.004429     -1  \n",
       "...             ...       ...    ...  \n",
       "33684       1.17055 -0.001604      0  \n",
       "33685       1.17018 -0.003067     -1  \n",
       "33686       1.16746 -0.005876     -1  \n",
       "33687       1.16750 -0.005587     -1  \n",
       "33688       1.16710 -0.005784     -1  \n",
       "\n",
       "[33689 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data_path = 'data/FTH_eurusd.csv'\n",
    "\n",
    "df = pd.read_csv(labeled_data_path)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09391677",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 24\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(df) - WINDOW_SIZE):\n",
    "    window = df['Close'].iloc[i:i + WINDOW_SIZE].values\n",
    "    label = df['Label'].iloc[i + WINDOW_SIZE - 1] \n",
    "    \n",
    "    X.append(window)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81714226",
   "metadata": {},
   "outputs": [],
   "source": [
    "gasf = GramianAngularField(method='summation', image_size=WINDOW_SIZE)\n",
    "gADF = GramianAngularField(method='difference', image_size=WINDOW_SIZE)\n",
    "rp = RecurrencePlot(threshold=None)\n",
    "#mtf = MarkovTransitionField(image_size=X.shape[1], n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ce0c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {-1: 'Downward', 0: 'Sideway', 1: 'Upward'}\n",
    "\n",
    "gasf_dir = 'datasets/FTH/EURUSD/GASF'\n",
    "gADF_dir = 'datasets/FTH/EURUSD/GADF'\n",
    "rp_dir = 'datasets/FTH/EURUSD/RP'\n",
    "\n",
    "for label_name in label_map.values():\n",
    "    os.makedirs(os.path.join(gasf_dir, label_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(gADF_dir, label_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(rp_dir, label_name), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e011c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on feature projection (RP): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33665/33665 [01:41<00:00, 331.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Memory usage issue\n",
    "# # Run Once\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# transformers = [\n",
    "#     (gasf, gasf_dir, \"GASF\"),\n",
    "#     (gADF, gADF_dir, \"GADF\"),\n",
    "#     (rp, rp_dir, \"RP\"), \n",
    "# ]\n",
    "\n",
    "# for transformer, save_dir, name in transformers:\n",
    "#     print(f\"\\nRunning on feature projection ({name}): \")\n",
    "\n",
    "#     for i, (series, label) in enumerate(tqdm(zip(X, y), total=len(X)), start=1):\n",
    "#         image = transformer.fit_transform(series.reshape(1, -1))[0]\n",
    "#         label_name = label_map[label]\n",
    "#         image_path = os.path.join(save_dir, label_name, f'{label_name}_{i}.png')\n",
    "\n",
    "#         plt.imsave(image_path, image, cmap='rainbow')\n",
    "#         plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "661b6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import os \n",
    "import torch\n",
    "\n",
    "class MultiFeatureFusionDataset(Dataset):\n",
    "    def __init__(self, root_dir, class_to_idx, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.data_list = self._make_data_list()\n",
    "\n",
    "        if not self.data_list:\n",
    "            raise RuntimeError(f\"Error :: Not found a data in Path: {root_dir}. Check the directory structure.\")\n",
    "        \n",
    "    def _make_data_list(self):\n",
    "        data_list = []\n",
    "\n",
    "        classes = list(self.class_to_idx.keys())\n",
    "\n",
    "        for class_name in classes:\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "            dir = os.path.join(self.root_dir, 'GASF', class_name)\n",
    "            files = sorted(glob(os.path.join(dir, '*.png')))\n",
    "\n",
    "            for path in files:\n",
    "                filename = os.path.basename(path)\n",
    "\n",
    "                gadf_path = os.path.join(self.root_dir, 'GADF', class_name, filename)\n",
    "                rp_path = os.path.join(self.root_dir, 'RP', class_name, filename)\n",
    "\n",
    "                if os.path.exists(gadf_path) and os.path.exists(rp_path):\n",
    "                    data_list.append({\n",
    "                        'gasf': path,\n",
    "                        'gadf': gadf_path,\n",
    "                        'rp': rp_path,\n",
    "                        'label': class_idx\n",
    "                    })\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data_list[index]\n",
    "\n",
    "        img_gasf = Image.open(item['gasf']).convert('RGB') # L\n",
    "        img_gadf = Image.open(item['gadf']).convert('RGB')\n",
    "        img_rp = Image.open(item['rp']).convert('RGB')\n",
    "\n",
    "        label = item['label']\n",
    "\n",
    "        if self.transform:\n",
    "            img_gasf = self.transform(img_gasf)\n",
    "            img_gadf = self.transform(img_gadf)\n",
    "            img_rp = self.transform(img_rp)\n",
    "\n",
    "        return img_gasf, img_gadf, img_rp, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d5c25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleFeatureFusionDataset(Dataset):\n",
    "    def __init__(self, root_dir, f1, f2, class_to_idx, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.f1 = f1\n",
    "        self.f2 = f2\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.data_list = self._make_data_list()\n",
    "\n",
    "        if not self.data_list:\n",
    "            raise RuntimeError(f\"Error :: Not found a data in Path: {root_dir}. Check the directory structure.\")\n",
    "        \n",
    "    def _make_data_list(self):\n",
    "        data_list = []\n",
    "\n",
    "        classes = list(self.class_to_idx.keys())\n",
    "\n",
    "        for class_name in classes:\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "            dir = os.path.join(self.root_dir, self.f1, class_name)\n",
    "            files = sorted(glob(os.path.join(dir, '*.png')))\n",
    "\n",
    "            for path in files:\n",
    "                filename = os.path.basename(path)\n",
    "                f2_path = os.path.join(self.root_dir, self.f2, class_name, filename)\n",
    "                \n",
    "                if os.path.exists(f2_path):\n",
    "                    data_list.append({\n",
    "                        'f1': path,\n",
    "                        'f2': f2_path,\n",
    "                        'label': class_idx\n",
    "                    })\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data_list[index]\n",
    "\n",
    "        img_f1 = Image.open(item['f1']).convert('RGB') # L\n",
    "        img_f2 = Image.open(item['f2']).convert('RGB')\n",
    "\n",
    "        label = item['label']\n",
    "\n",
    "        if self.transform:\n",
    "            img_f1 = self.transform(img_f1)\n",
    "            img_f2 = self.transform(img_f2)\n",
    "\n",
    "        return img_f1, img_f2, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc8abb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing()\n",
    "])\n",
    "eurusd_data_root = 'datasets/FTH/EURUSD_sample'\n",
    "class_to_idx = {\n",
    "    'Downward': -1,\n",
    "    'Sideway': 0,\n",
    "    'Upward': 1\n",
    "}\n",
    "eurusd_dataset = MultiFeatureFusionDataset(\n",
    "    root_dir=eurusd_data_root,\n",
    "    transform=transform,\n",
    "    class_to_idx=class_to_idx\n",
    ")\n",
    "\n",
    "eurusd_npz_path = 'datasets/FTH/Serialized_FTH_EURUSD_sample.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def serialize_multifeaturefusion(dataset, output_npz_path):    \n",
    "    data_list = dataset.data_list\n",
    "    \n",
    "    if not data_list:\n",
    "        print(\"Warning: There is no data to serialize.\")\n",
    "        return\n",
    "    \n",
    "    if os.path.exists(output_npz_path):\n",
    "        print(\"Already exists the serialized dataset for NPZ.\")\n",
    "        return\n",
    "    \n",
    "    gasf_arrays = []\n",
    "    gadf_arrays = []\n",
    "    rp_arrays = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Start to serialize {len(data_list)} samples for multi-features fusion...\")\n",
    "    \n",
    "    for item in tqdm(data_list, desc=\"Serializing\"):\n",
    "        img_gasf = Image.open(item['gasf']).convert('RGB')\n",
    "        img_gadf = Image.open(item['gadf']).convert('RGB')\n",
    "        img_rp = Image.open(item['rp']).convert('RGB')\n",
    "\n",
    "        arr_gasf = np.array(img_gasf, dtype=np.uint8)\n",
    "        arr_gadf = np.array(img_gadf, dtype=np.uint8)\n",
    "        arr_rp = np.array(img_rp, dtype=np.uint8)\n",
    "            \n",
    "        gasf_arrays.append(arr_gasf)\n",
    "        gadf_arrays.append(arr_gadf)\n",
    "        rp_arrays.append(arr_rp)\n",
    "        labels.append(item['label'])\n",
    "\n",
    "    final_gasf = np.array(gasf_arrays)\n",
    "    final_gadf = np.array(gadf_arrays)\n",
    "    final_rp = np.array(rp_arrays)\n",
    "    final_labels = np.array(labels, dtype=np.int64)\n",
    "    \n",
    "    np.savez_compressed(\n",
    "        output_npz_path,\n",
    "        gasf=final_gasf,\n",
    "        gadf=final_gadf,\n",
    "        rp=final_rp,\n",
    "        labels=final_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Complete to serialize! Path to save the file: {output_npz_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f12599f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_doublefeaturefusion(dataset, output_npz_path, feature_1, feature_2):    \n",
    "    data_list = dataset.data_list\n",
    "    \n",
    "    if not data_list:\n",
    "        print(\"Warning: There is no data to serialize.\")\n",
    "        return\n",
    "    \n",
    "    if os.path.exists(output_npz_path):\n",
    "        print(\"Already exists the serialized dataset for NPZ.\")\n",
    "        return\n",
    "\n",
    "    f1_arrays = []\n",
    "    f2_arrays = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Start to serialize {len(data_list)} samples for double-features fusion...\")\n",
    "    \n",
    "    for item in tqdm(data_list, desc=\"Serializing\"):\n",
    "        f1 = Image.open(item[feature_1]).convert('RGB')\n",
    "        f2 = Image.open(item[feature_2]).convert('RGB')\n",
    "\n",
    "        arr_f1 = np.array(f1, dtype=np.uint8)\n",
    "        arr_f2 = np.array(f2, dtype=np.uint8)\n",
    "            \n",
    "        f1_arrays.append(arr_f1)\n",
    "        f2_arrays.append(arr_f2)\n",
    "        labels.append(item['label'])\n",
    "\n",
    "    final_f1 = np.array(f1_arrays)\n",
    "    final_f2 = np.array(f2_arrays)\n",
    "    final_labels = np.array(labels, dtype=np.int64)\n",
    "    \n",
    "    np.savez_compressed(\n",
    "        output_npz_path,\n",
    "        f1=final_f1,\n",
    "        f2=final_f2,\n",
    "        labels=final_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Complete to serialize! Path to save the file: {output_npz_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08df73a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to serialize 9622 samples for multi-features fusion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serializing: 100%|██████████| 9622/9622 [00:32<00:00, 299.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete to serialize! Path to save the file: datasets/FTH/Serialized_FTH_EURUSD_sample.npz\n"
     ]
    }
   ],
   "source": [
    "serialize_multifeaturefusion(eurusd_dataset, eurusd_npz_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42ae0ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to serialize 9622 samples for double-features fusion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serializing: 100%|██████████| 9622/9622 [00:05<00:00, 1722.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete to serialize! Path to save the file: datasets/FTH\\Serialized_FTH_EURUSD_sample_gasf_gadf.npz\n"
     ]
    }
   ],
   "source": [
    "npz_path = 'datasets/FTH'\n",
    "feature_name1 = 'gasf'\n",
    "feature_name2 = 'gadf'\n",
    "file_name = 'Serialized_FTH_EURUSD_sample_' + feature_name1 + '_' + feature_name2 + '.npz'\n",
    "npz_path_to_save = os.path.join(npz_path, file_name)\n",
    "\n",
    "serialize_doublefeaturefusion(eurusd_dataset, npz_path_to_save, feature_name1, feature_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf085a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
